{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP3+5xeTf/Ky+8GYLd0YA11",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sr606/LLM/blob/main/mermaid_trail_v3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install graphviz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z7CaV5imv0tH",
        "outputId": "f4512c64-f7ae-4bcc-a9d3-492da8e5915b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.12/dist-packages (0.21)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KFBRBFo0vPLV",
        "outputId": "ed5f19b1-ae1c-4d05-a1bb-eb04b0193574"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ JSON exported as etl_metadata.json\n",
            "‚úÖ Diagram generated as etl_flow_detailed.pdf\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import json\n",
        "from graphviz import Digraph\n",
        "from collections import defaultdict, deque\n",
        "\n",
        "\n",
        "# =====================================================\n",
        "# 1Ô∏è‚É£ CLASSIFY STAGE TYPE\n",
        "# =====================================================\n",
        "\n",
        "def classify_layer(stage_type):\n",
        "    st = stage_type.upper()\n",
        "\n",
        "    if \"TRANSFORMER\" in st:\n",
        "        return \"TRANSFORM\"\n",
        "\n",
        "    if \"HASHED\" in st:\n",
        "        return \"HASH\"\n",
        "\n",
        "    if \"SEQ\" in st:\n",
        "        return \"FILE\"\n",
        "\n",
        "    if \"ORACLE\" in st or \"CUSTOM\" in st:\n",
        "        return \"DB\"\n",
        "\n",
        "    return \"OTHER\"\n",
        "\n",
        "\n",
        "# =====================================================\n",
        "# 2Ô∏è‚É£ PARSE PSEUDOCODE INTO METADATA\n",
        "# =====================================================\n",
        "\n",
        "def parse_pseudocode(text):\n",
        "\n",
        "    stages = {}\n",
        "    dataset_producers = {}\n",
        "\n",
        "    stage_pattern = r\"--- \\[(.*?) : (.*?)\\]\"\n",
        "    input_pattern = r\"Input:\\s*‚Üê\\s*(dataset_\\d+)\"\n",
        "    output_pattern = r\"Output:\\s*‚Üí\\s*(dataset_\\d+)\"\n",
        "    stagevar_pattern = r\"StageVar (.*)\"\n",
        "    constraint_pattern = r\"Constraint \\((.*?)\\): (.*)\"\n",
        "    table_pattern = r\"\\bFROM\\s+([\\w\\.]+)|\\bJOIN\\s+([\\w\\.]+)\"\n",
        "\n",
        "    current_stage = None\n",
        "    collecting_sql = False\n",
        "\n",
        "    for line in text.splitlines():\n",
        "\n",
        "        stage_match = re.search(stage_pattern, line)\n",
        "        if stage_match:\n",
        "            stage_type = stage_match.group(1).strip()\n",
        "            stage_name = stage_match.group(2).strip()\n",
        "\n",
        "            current_stage = stage_name\n",
        "            collecting_sql = False\n",
        "\n",
        "            stages[current_stage] = {\n",
        "                \"type\": stage_type,\n",
        "                \"layer\": classify_layer(stage_type),\n",
        "                \"inputs\": [],\n",
        "                \"outputs\": [],\n",
        "                \"tables\": [],\n",
        "                \"stagevars\": [],\n",
        "                \"constraints\": [],\n",
        "                \"joins\": 0,\n",
        "                \"logic_complexity\": 0\n",
        "            }\n",
        "            continue\n",
        "\n",
        "        if current_stage:\n",
        "\n",
        "            # Detect SQL start\n",
        "            if \"SQL:\" in line:\n",
        "                collecting_sql = True\n",
        "                continue\n",
        "\n",
        "            # Collect SQL lines\n",
        "            if collecting_sql:\n",
        "                tables = re.findall(table_pattern, line, re.IGNORECASE)\n",
        "                for t in tables:\n",
        "                    for table in t:\n",
        "                        if table:\n",
        "                            stages[current_stage][\"tables\"].append(table)\n",
        "\n",
        "                if \"JOIN\" in line.upper():\n",
        "                    stages[current_stage][\"joins\"] += 1\n",
        "\n",
        "                if \"CASE\" in line.upper() or \"IF\" in line.upper():\n",
        "                    stages[current_stage][\"logic_complexity\"] += 1\n",
        "\n",
        "            # Inputs\n",
        "            input_match = re.search(input_pattern, line)\n",
        "            if input_match:\n",
        "                stages[current_stage][\"inputs\"].append(input_match.group(1))\n",
        "\n",
        "            # Outputs\n",
        "            output_match = re.search(output_pattern, line)\n",
        "            if output_match:\n",
        "                dataset = output_match.group(1)\n",
        "                stages[current_stage][\"outputs\"].append(dataset)\n",
        "                dataset_producers[dataset] = current_stage\n",
        "\n",
        "            # Stage variables\n",
        "            stagevar_match = re.search(stagevar_pattern, line)\n",
        "            if stagevar_match:\n",
        "                stages[current_stage][\"stagevars\"].append(stagevar_match.group(1))\n",
        "\n",
        "            # Constraints\n",
        "            constraint_match = re.search(constraint_pattern, line)\n",
        "            if constraint_match:\n",
        "                stages[current_stage][\"constraints\"].append(constraint_match.group(2))\n",
        "\n",
        "    return stages, dataset_producers\n",
        "\n",
        "\n",
        "# =====================================================\n",
        "# 3Ô∏è‚É£ COMPUTE EXECUTION DEPTH\n",
        "# =====================================================\n",
        "\n",
        "def compute_depths(stages, dataset_producers):\n",
        "\n",
        "    graph = defaultdict(list)\n",
        "    indegree = defaultdict(int)\n",
        "\n",
        "    for stage, info in stages.items():\n",
        "        for dataset in info[\"inputs\"]:\n",
        "            if dataset in dataset_producers:\n",
        "                parent = dataset_producers[dataset]\n",
        "                graph[parent].append(stage)\n",
        "                indegree[stage] += 1\n",
        "\n",
        "    depth = {}\n",
        "    queue = deque()\n",
        "\n",
        "    for stage in stages:\n",
        "        if indegree[stage] == 0:\n",
        "            queue.append(stage)\n",
        "            depth[stage] = 0\n",
        "\n",
        "    while queue:\n",
        "        node = queue.popleft()\n",
        "        for neighbor in graph[node]:\n",
        "            indegree[neighbor] -= 1\n",
        "            if indegree[neighbor] == 0:\n",
        "                depth[neighbor] = depth[node] + 1\n",
        "                queue.append(neighbor)\n",
        "\n",
        "    return depth\n",
        "\n",
        "\n",
        "# =====================================================\n",
        "# 4Ô∏è‚É£ BUILD GRAPHVIZ FROM METADATA\n",
        "# =====================================================\n",
        "\n",
        "def build_graphviz(stages, dataset_producers):\n",
        "\n",
        "    dot = Digraph(\"ETL_Flow\", engine=\"dot\")\n",
        "    dot.attr(rankdir=\"LR\", splines=\"spline\", nodesep=\"0.8\", ranksep=\"1.2\")\n",
        "\n",
        "    dot.attr(\"node\",\n",
        "             shape=\"box\",\n",
        "             style=\"rounded,filled\",\n",
        "             width=\"3\",\n",
        "             height=\"1\")\n",
        "\n",
        "    depths = compute_depths(stages, dataset_producers)\n",
        "\n",
        "    # Detect final targets\n",
        "    outgoing = defaultdict(int)\n",
        "    for stage, info in stages.items():\n",
        "        for dataset in info[\"inputs\"]:\n",
        "            if dataset in dataset_producers:\n",
        "                parent = dataset_producers[dataset]\n",
        "                outgoing[parent] += 1\n",
        "\n",
        "    targets = [s for s in stages if outgoing[s] == 0]\n",
        "\n",
        "    max_depth = max(depths.values())\n",
        "    for t in targets:\n",
        "        if stages[t][\"layer\"] == \"DB\":\n",
        "            depths[t] = max_depth + 1\n",
        "\n",
        "    # Group by depth\n",
        "    levels = defaultdict(list)\n",
        "    for stage, d in depths.items():\n",
        "        levels[d].append(stage)\n",
        "\n",
        "    # Create nodes\n",
        "    for d in sorted(levels):\n",
        "        with dot.subgraph() as s:\n",
        "            s.attr(rank=\"same\")\n",
        "            for stage in levels[d]:\n",
        "                info = stages[stage]\n",
        "\n",
        "                label = f\"{stage}\\n\"\n",
        "                label += f\"Type: {info['type']}\\n\"\n",
        "                label += f\"In: {len(info['inputs'])} | Out: {len(info['outputs'])}\\n\"\n",
        "\n",
        "                if info[\"tables\"]:\n",
        "                    label += f\"Tables: {len(set(info['tables']))}\\n\"\n",
        "\n",
        "                if info[\"joins\"] > 0:\n",
        "                    label += f\"Joins: {info['joins']}\\n\"\n",
        "\n",
        "                if info[\"stagevars\"]:\n",
        "                    label += f\"StageVars: {len(info['stagevars'])}\\n\"\n",
        "\n",
        "                if info[\"constraints\"]:\n",
        "                    label += f\"Constraints: {len(info['constraints'])}\"\n",
        "\n",
        "                color = \"#F4F6F7\"\n",
        "                if info[\"layer\"] == \"DB\":\n",
        "                    color = \"#AED6F1\"\n",
        "                elif info[\"layer\"] == \"TRANSFORM\":\n",
        "                    color = \"#F9E79F\"\n",
        "                elif info[\"layer\"] == \"HASH\":\n",
        "                    color = \"#ABEBC6\"\n",
        "                elif info[\"layer\"] == \"FILE\":\n",
        "                    color = \"#F5B7B1\"\n",
        "\n",
        "                s.node(stage, label, fillcolor=color)\n",
        "\n",
        "    # Add edges\n",
        "    for stage, info in stages.items():\n",
        "        for dataset in info[\"inputs\"]:\n",
        "            if dataset in dataset_producers:\n",
        "                parent = dataset_producers[dataset]\n",
        "\n",
        "                if stages[parent][\"layer\"] == \"HASH\":\n",
        "                    dot.edge(parent, stage,\n",
        "                             constraint=\"false\",\n",
        "                             color=\"gray\")\n",
        "                else:\n",
        "                    dot.edge(parent, stage)\n",
        "\n",
        "    return dot\n",
        "\n",
        "\n",
        "# =====================================================\n",
        "# 5Ô∏è‚É£ RUN + EXPORT\n",
        "# =====================================================\n",
        "\n",
        "with open(\"Samle_Job2 1 2_detailed_pseudocode.txt\", \"r\") as f:\n",
        "    pseudo_text = f.read()\n",
        "\n",
        "stages, producers = parse_pseudocode(pseudo_text)\n",
        "\n",
        "# Export JSON metadata\n",
        "metadata = {\n",
        "    \"stages\": stages,\n",
        "    \"dependencies\": producers\n",
        "}\n",
        "\n",
        "with open(\"etl_metadata.json\", \"w\") as f:\n",
        "    json.dump(metadata, f, indent=4)\n",
        "\n",
        "# Build diagram\n",
        "graph = build_graphviz(stages, producers)\n",
        "graph.render(\"etl_flow_detailed\", format=\"pdf\")\n",
        "\n",
        "print(\"‚úÖ JSON exported as etl_metadata.json\")\n",
        "print(\"‚úÖ Diagram generated as etl_flow_detailed.pdf\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import json\n",
        "from collections import defaultdict\n",
        "\n",
        "\n",
        "# ==========================================================\n",
        "# 1Ô∏è‚É£ CLASSIFY STAGE LAYER\n",
        "# ==========================================================\n",
        "\n",
        "def classify_layer(stage_type):\n",
        "    st = stage_type.upper()\n",
        "\n",
        "    if \"TRANSFORMER\" in st:\n",
        "        return \"Transform\"\n",
        "\n",
        "    if \"HASHED\" in st:\n",
        "        return \"Intermediate\"\n",
        "\n",
        "    if \"SEQ\" in st:\n",
        "        return \"Outputs\"\n",
        "\n",
        "    if \"ORACLE\" in st or \"CUSTOM\" in st:\n",
        "        return \"Source_Target\"\n",
        "\n",
        "    return \"Other\"\n",
        "\n",
        "\n",
        "# ==========================================================\n",
        "# 2Ô∏è‚É£ PARSE PSEUDOCODE\n",
        "# ==========================================================\n",
        "\n",
        "def parse_pseudocode(text):\n",
        "\n",
        "    stages = {}\n",
        "    dataset_producers = {}\n",
        "\n",
        "    stage_pattern = r\"--- \\[(.*?) : (.*?)\\]\"\n",
        "    input_pattern = r\"Input:\\s*‚Üê\\s*(dataset_\\d+)\\s*\\((.*?)\\)\"\n",
        "    output_pattern = r\"Output:\\s*‚Üí\\s*(dataset_\\d+)\\s*\\((.*?)\\)\"\n",
        "    stagevar_pattern = r\"StageVar (.*)\"\n",
        "    constraint_pattern = r\"Constraint \\((.*?)\\): (.*)\"\n",
        "    table_pattern = r\"\\bFROM\\s+([\\w\\.]+)|\\bJOIN\\s+([\\w\\.]+)\"\n",
        "\n",
        "    current_stage = None\n",
        "    collecting_sql = False\n",
        "\n",
        "    for line in text.splitlines():\n",
        "\n",
        "        stage_match = re.search(stage_pattern, line)\n",
        "        if stage_match:\n",
        "            stage_type = stage_match.group(1).strip()\n",
        "            stage_name = stage_match.group(2).strip()\n",
        "\n",
        "            current_stage = stage_name\n",
        "            collecting_sql = False\n",
        "\n",
        "            stages[current_stage] = {\n",
        "                \"type\": stage_type,\n",
        "                \"layer\": classify_layer(stage_type),\n",
        "                \"inputs\": [],\n",
        "                \"outputs\": [],\n",
        "                \"stagevars\": [],\n",
        "                \"constraints\": [],\n",
        "                \"tables\": [],\n",
        "                \"joins\": 0,\n",
        "                \"business_rules\": []\n",
        "            }\n",
        "            continue\n",
        "\n",
        "        if not current_stage:\n",
        "            continue\n",
        "\n",
        "        if \"SQL:\" in line:\n",
        "            collecting_sql = True\n",
        "            continue\n",
        "\n",
        "        if collecting_sql:\n",
        "            tables = re.findall(table_pattern, line, re.IGNORECASE)\n",
        "            for t in tables:\n",
        "                for table in t:\n",
        "                    if table:\n",
        "                        stages[current_stage][\"tables\"].append(table)\n",
        "\n",
        "            if \"JOIN\" in line.upper():\n",
        "                stages[current_stage][\"joins\"] += 1\n",
        "\n",
        "        input_match = re.search(input_pattern, line)\n",
        "        if input_match:\n",
        "            dataset = input_match.group(1)\n",
        "            dataset_name = input_match.group(2)\n",
        "            stages[current_stage][\"inputs\"].append((dataset, dataset_name))\n",
        "\n",
        "        output_match = re.search(output_pattern, line)\n",
        "        if output_match:\n",
        "            dataset = output_match.group(1)\n",
        "            dataset_name = output_match.group(2)\n",
        "            stages[current_stage][\"outputs\"].append((dataset, dataset_name))\n",
        "            dataset_producers[dataset] = current_stage\n",
        "\n",
        "        stagevar_match = re.search(stagevar_pattern, line)\n",
        "        if stagevar_match:\n",
        "            rule = stagevar_match.group(1).strip()\n",
        "            stages[current_stage][\"stagevars\"].append(rule)\n",
        "            stages[current_stage][\"business_rules\"].append(rule)\n",
        "\n",
        "        constraint_match = re.search(constraint_pattern, line)\n",
        "        if constraint_match:\n",
        "            rule = constraint_match.group(2).strip()\n",
        "            stages[current_stage][\"constraints\"].append(rule)\n",
        "            stages[current_stage][\"business_rules\"].append(rule)\n",
        "\n",
        "        if \" IF \" in line.upper():\n",
        "            stages[current_stage][\"business_rules\"].append(line.strip())\n",
        "\n",
        "    return stages, dataset_producers\n",
        "\n",
        "\n",
        "# ==========================================================\n",
        "# 3Ô∏è‚É£ GENERATE MERMAID CODE\n",
        "# ==========================================================\n",
        "\n",
        "def generate_mermaid(stages, producers):\n",
        "\n",
        "    lines = []\n",
        "    lines.append(\"flowchart LR\")\n",
        "\n",
        "    layers = defaultdict(list)\n",
        "    for stage, info in stages.items():\n",
        "        layers[info[\"layer\"]].append(stage)\n",
        "\n",
        "    for layer, stage_list in layers.items():\n",
        "        lines.append(f\"  subgraph {layer}\")\n",
        "        for stage in stage_list:\n",
        "            info = stages[stage]\n",
        "\n",
        "            label = f\"{info['type']}: {stage}\"\n",
        "            if info[\"joins\"] > 0:\n",
        "                label += f\"<br/>Joins: {info['joins']}\"\n",
        "            if info[\"stagevars\"]:\n",
        "                label += f\"<br/>StageVars: {len(info['stagevars'])}\"\n",
        "            if info[\"constraints\"]:\n",
        "                label += f\"<br/>Constraints: {len(info['constraints'])}\"\n",
        "\n",
        "            lines.append(f'    {stage}[\"{label}\"]')\n",
        "        lines.append(\"  end\\n\")\n",
        "\n",
        "    for stage, info in stages.items():\n",
        "        for dataset, dataset_name in info[\"inputs\"]:\n",
        "            parent = producers.get(dataset)\n",
        "            if parent:\n",
        "                lines.append(\n",
        "                    f'  {parent} -- \"{dataset}: {dataset_name}\" --> {stage}'\n",
        "                )\n",
        "\n",
        "    return \"\\n\".join(lines)\n",
        "\n",
        "\n",
        "# ==========================================================\n",
        "# 4Ô∏è‚É£ EXPORT TO PDF-READY HTML\n",
        "# ==========================================================\n",
        "\n",
        "with open(\"Sample_Job1 1 2_detailed_pseudocode.txt\", \"r\") as f:\n",
        "    pseudo_text = f.read()\n",
        "\n",
        "stages, producers = parse_pseudocode(pseudo_text)\n",
        "mermaid_code = generate_mermaid(stages, producers)\n",
        "\n",
        "html_template = f\"\"\"\n",
        "<!DOCTYPE html>\n",
        "<html>\n",
        "<head>\n",
        "  <script src=\"https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js\"></script>\n",
        "  <script>mermaid.initialize({{ startOnLoad: true }});</script>\n",
        "</head>\n",
        "<body>\n",
        "<div class=\"mermaid\">\n",
        "{mermaid_code}\n",
        "</div>\n",
        "</body>\n",
        "</html>\n",
        "\"\"\"\n",
        "\n",
        "with open(\"etl_flow.html\", \"w\") as f:\n",
        "    f.write(html_template)\n",
        "\n",
        "print(\"HTML generated ‚Üí etl_flow.html\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GMZRcFCiyFju",
        "outputId": "21edf3fe-ac62-42e0-a4ae-3e8920df5036"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ HTML generated ‚Üí etl_flow.html\n",
            "üëâ Open in browser ‚Üí Print ‚Üí Save as PDF\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CGRHXpfiyGF0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bSeroZPfyGI4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dbCusboXyGMW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}