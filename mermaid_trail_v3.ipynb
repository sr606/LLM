{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMaEeluqWIWjudfneXK0maS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sr606/LLM/blob/main/mermaid_trail_v3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#genearte_business_lineage\n",
        "import json\n",
        "from graphviz import Digraph\n",
        "\n",
        "\n",
        "with open(\"business_lineage.json\", \"r\") as f:\n",
        "    model = json.load(f)\n",
        "\n",
        "\n",
        "dot = Digraph(\"Business_Lineage\", format=\"pdf\")\n",
        "dot.attr(rankdir=\"LR\", fontsize=\"11\")\n",
        "dot.attr(\"node\", shape=\"box\", style=\"rounded,filled\")\n",
        "\n",
        "\n",
        "# Color scheme\n",
        "COLORS = {\n",
        "    \"source\": \"#E6F2FF\",\n",
        "    \"dimension\": \"#FFF2CC\",\n",
        "    \"fact\": \"#E6FFE6\",\n",
        "    \"exception\": \"#FDEDEC\"\n",
        "}\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Source Node\n",
        "# -----------------------------\n",
        "source = model[\"sources\"][0][\"table\"]\n",
        "dot.node(\"source\", source, fillcolor=COLORS[\"source\"])\n",
        "\n",
        "\n",
        "previous_node = \"source\"\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Dimension Nodes\n",
        "# -----------------------------\n",
        "for i, dim in enumerate(model.get(\"dimensions\", [])):\n",
        "    node_id = f\"dim_{i}\"\n",
        "\n",
        "    label = f\"\"\"{dim[\"table\"]}\\n\n",
        "{dim[\"join_type\"]}\\n\n",
        "ON {dim[\"condition\"]}\"\"\"\n",
        "\n",
        "    dot.node(node_id, label, fillcolor=COLORS[\"dimension\"])\n",
        "\n",
        "    dot.edge(previous_node, node_id)\n",
        "    previous_node = node_id\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Fact Target\n",
        "# -----------------------------\n",
        "fact = model[\"fact_target\"][\"table\"]\n",
        "dot.node(\"fact\", fact, fillcolor=COLORS[\"fact\"])\n",
        "\n",
        "dot.edge(previous_node, \"fact\")\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Exception Branch\n",
        "# -----------------------------\n",
        "if model.get(\"exception_output\"):\n",
        "    exc = model[\"exception_output\"]\n",
        "    dot.node(\"exception\",\n",
        "             f\"{exc['name']}\\nTrigger: {exc['trigger']}\",\n",
        "             fillcolor=COLORS[\"exception\"])\n",
        "\n",
        "    dot.edge(\"fact\", \"exception\")\n",
        "\n",
        "\n",
        "dot.render(\"business_lineage\", view=True)\n",
        "\n",
        "print(\"‚úÖ business_lineage.pdf generated\")\n"
      ],
      "metadata": {
        "id": "m9qnloHfqQLK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BUSINESS_LINEAGE_PROMPT = \"\"\"\n",
        "You are a senior data architect.\n",
        "\n",
        "Extract BUSINESS-LEVEL lineage only.\n",
        "\n",
        "Ignore:\n",
        "- Technical hash stages\n",
        "- Internal dataset names\n",
        "- Lookup helper tables\n",
        "- Intermediate technical objects\n",
        "\n",
        "Return JSON with:\n",
        "\n",
        "{\n",
        "  \"pipeline_name\": \"\",\n",
        "  \"sources\": [],\n",
        "  \"dimensions\": [],\n",
        "  \"fact_target\": {},\n",
        "  \"exception_output\": {},\n",
        "  \"business_rules\": []\n",
        "}\n",
        "\n",
        "Only include:\n",
        "- Primary source tables\n",
        "- Major business dimension joins\n",
        "- Join type\n",
        "- Join condition\n",
        "- Final fact target\n",
        "- Exception outputs\n",
        "- Important transformation rules\n",
        "\n",
        "Return valid JSON only.\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "87UR3N5Jqbgf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import html\n",
        "from graphviz import Digraph\n",
        "\n",
        "\n",
        "# =====================================================\n",
        "# 1Ô∏è‚É£ Load Pipeline Model\n",
        "# =====================================================\n",
        "\n",
        "with open(\"etl_pipeline_model.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    model = json.load(f)\n",
        "\n",
        "\n",
        "# =====================================================\n",
        "# 2Ô∏è‚É£ Safe Text Utility\n",
        "# =====================================================\n",
        "\n",
        "def safe(text):\n",
        "    if text is None:\n",
        "        return \"\"\n",
        "    return html.escape(str(text))\n",
        "\n",
        "\n",
        "def format_section(title, lines):\n",
        "    if not lines:\n",
        "        return \"\"\n",
        "\n",
        "    section = f\"<TR><TD ALIGN='LEFT'><B>{safe(title)}</B></TD></TR>\"\n",
        "    for line in lines:\n",
        "        section += f\"<TR><TD ALIGN='LEFT'>‚Ä¢ {safe(line)}</TD></TR>\"\n",
        "    return section\n",
        "\n",
        "\n",
        "# =====================================================\n",
        "# 3Ô∏è‚É£ Structured Formatters\n",
        "# =====================================================\n",
        "\n",
        "def format_input(inp):\n",
        "    lines = []\n",
        "\n",
        "    if isinstance(inp, dict):\n",
        "        if inp.get(\"name\"):\n",
        "            lines.append(f\"Stage: {inp['name']}\")\n",
        "        if inp.get(\"type\"):\n",
        "            lines.append(f\"Type: {inp['type']}\")\n",
        "        if inp.get(\"query\"):\n",
        "            lines.append(\"Query: SQL Extract\")\n",
        "    else:\n",
        "        lines.append(str(inp))\n",
        "\n",
        "    return lines\n",
        "\n",
        "\n",
        "def format_process(proc):\n",
        "    lines = []\n",
        "\n",
        "    if isinstance(proc, dict):\n",
        "        if proc.get(\"name\"):\n",
        "            lines.append(f\"Stage: {proc['name']}\")\n",
        "        if proc.get(\"type\"):\n",
        "            lines.append(f\"Type: {proc['type']}\")\n",
        "\n",
        "        if proc.get(\"transformations\"):\n",
        "            for t in proc[\"transformations\"][:2]:\n",
        "                if isinstance(t, dict):\n",
        "                    lines.append(f\"Output: {t.get('output','')}\")\n",
        "    else:\n",
        "        lines.append(str(proc))\n",
        "\n",
        "    return lines\n",
        "\n",
        "\n",
        "def format_join(join):\n",
        "    lines = []\n",
        "\n",
        "    if isinstance(join, dict):\n",
        "        if join.get(\"type\"):\n",
        "            lines.append(f\"Join Type: {join['type']}\")\n",
        "\n",
        "        keys = join.get(\"keys\", [])\n",
        "        for k in keys[:4]:\n",
        "            lines.append(f\"ON {k}\")\n",
        "    else:\n",
        "        lines.append(str(join))\n",
        "\n",
        "    return lines\n",
        "\n",
        "\n",
        "def format_analytics(ana):\n",
        "    lines = []\n",
        "\n",
        "    if isinstance(ana, dict):\n",
        "        if ana.get(\"group_by\"):\n",
        "            lines.append(\"Group By:\")\n",
        "            for g in ana[\"group_by\"]:\n",
        "                lines.append(g)\n",
        "\n",
        "        if ana.get(\"metrics\"):\n",
        "            lines.append(\"Metrics:\")\n",
        "            for m in ana[\"metrics\"]:\n",
        "                lines.append(m)\n",
        "    else:\n",
        "        lines.append(str(ana))\n",
        "\n",
        "    return lines\n",
        "\n",
        "\n",
        "def format_output(out):\n",
        "    lines = []\n",
        "\n",
        "    if isinstance(out, dict):\n",
        "        if out.get(\"name\"):\n",
        "            lines.append(f\"Target: {out['name']}\")\n",
        "        if out.get(\"type\"):\n",
        "            lines.append(f\"Type: {out['type']}\")\n",
        "        if out.get(\"link_file\"):\n",
        "            lines.append(\"Mode: File Output\")\n",
        "    else:\n",
        "        lines.append(str(out))\n",
        "\n",
        "    return lines\n",
        "\n",
        "\n",
        "# =====================================================\n",
        "# 4Ô∏è‚É£ Create Graphviz Graph\n",
        "# =====================================================\n",
        "\n",
        "dot = Digraph(\"ETL_Technical_Architecture\", format=\"pdf\")\n",
        "dot.attr(rankdir=\"LR\", fontsize=\"10\")\n",
        "dot.attr(\"node\", shape=\"plaintext\")\n",
        "\n",
        "\n",
        "# =====================================================\n",
        "# 5Ô∏è‚É£ Build Pipelines\n",
        "# =====================================================\n",
        "\n",
        "for p_index, pipeline in enumerate(model.get(\"pipelines\", [])):\n",
        "\n",
        "    with dot.subgraph(name=f\"cluster_{p_index}\") as c:\n",
        "        c.attr(label=safe(pipeline.get(\"name\", f\"Pipeline_{p_index}\")),\n",
        "               fontsize=\"12\",\n",
        "               style=\"rounded\")\n",
        "\n",
        "        previous_node = None\n",
        "\n",
        "        def create_node(node_id, header, color, sections):\n",
        "            label = f\"\"\"\n",
        "            <\n",
        "            <TABLE BORDER=\"1\" CELLBORDER=\"0\" CELLSPACING=\"0\" CELLPADDING=\"6\">\n",
        "                <TR>\n",
        "                    <TD BGCOLOR=\"{color}\"><B>{safe(header)}</B></TD>\n",
        "                </TR>\n",
        "                {sections}\n",
        "            </TABLE>\n",
        "            >\n",
        "            \"\"\"\n",
        "\n",
        "            c.node(node_id, label=label)\n",
        "            return node_id\n",
        "\n",
        "        # --------------------\n",
        "        # INPUTS\n",
        "        # --------------------\n",
        "        for i, inp in enumerate(pipeline.get(\"inputs\", [])):\n",
        "            node_id = f\"{p_index}_IN_{i}\"\n",
        "            sections = format_section(\"Details\", format_input(inp))\n",
        "            current = create_node(node_id, \"INPUT\", \"#E6F2FF\", sections)\n",
        "\n",
        "            if previous_node:\n",
        "                c.edge(previous_node, current)\n",
        "            previous_node = current\n",
        "\n",
        "        # --------------------\n",
        "        # PROCESSES\n",
        "        # --------------------\n",
        "        for i, proc in enumerate(pipeline.get(\"processes\", [])):\n",
        "            node_id = f\"{p_index}_PROC_{i}\"\n",
        "            sections = format_section(\"Logic\", format_process(proc))\n",
        "            current = create_node(node_id, \"PROCESS\", \"#E6FFE6\", sections)\n",
        "\n",
        "            if previous_node:\n",
        "                c.edge(previous_node, current)\n",
        "            previous_node = current\n",
        "\n",
        "        # --------------------\n",
        "        # JOINS\n",
        "        # --------------------\n",
        "        for i, join in enumerate(pipeline.get(\"joins\", [])):\n",
        "            node_id = f\"{p_index}_JOIN_{i}\"\n",
        "            sections = format_section(\"Join Info\", format_join(join))\n",
        "            current = create_node(node_id, \"JOIN\", \"#FFF2CC\", sections)\n",
        "\n",
        "            if previous_node:\n",
        "                c.edge(previous_node, current)\n",
        "            previous_node = current\n",
        "\n",
        "        # --------------------\n",
        "        # ANALYTICS\n",
        "        # --------------------\n",
        "        for i, ana in enumerate(pipeline.get(\"analytics\", [])):\n",
        "            node_id = f\"{p_index}_ANA_{i}\"\n",
        "            sections = format_section(\"Analytics\", format_analytics(ana))\n",
        "            current = create_node(node_id, \"ANALYTICS\", \"#F4E6FF\", sections)\n",
        "\n",
        "            if previous_node:\n",
        "                c.edge(previous_node, current)\n",
        "            previous_node = current\n",
        "\n",
        "        # --------------------\n",
        "        # OUTPUTS\n",
        "        # --------------------\n",
        "        for i, out in enumerate(pipeline.get(\"outputs\", [])):\n",
        "            node_id = f\"{p_index}_OUT_{i}\"\n",
        "            sections = format_section(\"Target\", format_output(out))\n",
        "            current = create_node(node_id, \"OUTPUT\", \"#E6FFFF\", sections)\n",
        "\n",
        "            if previous_node:\n",
        "                c.edge(previous_node, current)\n",
        "            previous_node = current\n",
        "\n",
        "\n",
        "# =====================================================\n",
        "# 6Ô∏è‚É£ Render PDF\n",
        "# =====================================================\n",
        "\n",
        "dot.render(\"etl_technical_architecture\", view=True)\n",
        "\n",
        "print(\"‚úÖ Graphviz PDF generated ‚Üí etl_technical_architecture.pdf\")\n"
      ],
      "metadata": {
        "id": "LkvkRqYbjK7N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install graphviz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z7CaV5imv0tH",
        "outputId": "f4512c64-f7ae-4bcc-a9d3-492da8e5915b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.12/dist-packages (0.21)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KFBRBFo0vPLV",
        "outputId": "ed5f19b1-ae1c-4d05-a1bb-eb04b0193574"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ JSON exported as etl_metadata.json\n",
            "‚úÖ Diagram generated as etl_flow_detailed.pdf\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import json\n",
        "from graphviz import Digraph\n",
        "from collections import defaultdict, deque\n",
        "\n",
        "\n",
        "# =====================================================\n",
        "# 1Ô∏è‚É£ CLASSIFY STAGE TYPE\n",
        "# =====================================================\n",
        "\n",
        "def classify_layer(stage_type):\n",
        "    st = stage_type.upper()\n",
        "\n",
        "    if \"TRANSFORMER\" in st:\n",
        "        return \"TRANSFORM\"\n",
        "\n",
        "    if \"HASHED\" in st:\n",
        "        return \"HASH\"\n",
        "\n",
        "    if \"SEQ\" in st:\n",
        "        return \"FILE\"\n",
        "\n",
        "    if \"ORACLE\" in st or \"CUSTOM\" in st:\n",
        "        return \"DB\"\n",
        "\n",
        "    return \"OTHER\"\n",
        "\n",
        "\n",
        "# =====================================================\n",
        "# 2Ô∏è‚É£ PARSE PSEUDOCODE INTO METADATA\n",
        "# =====================================================\n",
        "\n",
        "def parse_pseudocode(text):\n",
        "\n",
        "    stages = {}\n",
        "    dataset_producers = {}\n",
        "\n",
        "    stage_pattern = r\"--- \\[(.*?) : (.*?)\\]\"\n",
        "    input_pattern = r\"Input:\\s*‚Üê\\s*(dataset_\\d+)\"\n",
        "    output_pattern = r\"Output:\\s*‚Üí\\s*(dataset_\\d+)\"\n",
        "    stagevar_pattern = r\"StageVar (.*)\"\n",
        "    constraint_pattern = r\"Constraint \\((.*?)\\): (.*)\"\n",
        "    table_pattern = r\"\\bFROM\\s+([\\w\\.]+)|\\bJOIN\\s+([\\w\\.]+)\"\n",
        "\n",
        "    current_stage = None\n",
        "    collecting_sql = False\n",
        "\n",
        "    for line in text.splitlines():\n",
        "\n",
        "        stage_match = re.search(stage_pattern, line)\n",
        "        if stage_match:\n",
        "            stage_type = stage_match.group(1).strip()\n",
        "            stage_name = stage_match.group(2).strip()\n",
        "\n",
        "            current_stage = stage_name\n",
        "            collecting_sql = False\n",
        "\n",
        "            stages[current_stage] = {\n",
        "                \"type\": stage_type,\n",
        "                \"layer\": classify_layer(stage_type),\n",
        "                \"inputs\": [],\n",
        "                \"outputs\": [],\n",
        "                \"tables\": [],\n",
        "                \"stagevars\": [],\n",
        "                \"constraints\": [],\n",
        "                \"joins\": 0,\n",
        "                \"logic_complexity\": 0\n",
        "            }\n",
        "            continue\n",
        "\n",
        "        if current_stage:\n",
        "\n",
        "            # Detect SQL start\n",
        "            if \"SQL:\" in line:\n",
        "                collecting_sql = True\n",
        "                continue\n",
        "\n",
        "            # Collect SQL lines\n",
        "            if collecting_sql:\n",
        "                tables = re.findall(table_pattern, line, re.IGNORECASE)\n",
        "                for t in tables:\n",
        "                    for table in t:\n",
        "                        if table:\n",
        "                            stages[current_stage][\"tables\"].append(table)\n",
        "\n",
        "                if \"JOIN\" in line.upper():\n",
        "                    stages[current_stage][\"joins\"] += 1\n",
        "\n",
        "                if \"CASE\" in line.upper() or \"IF\" in line.upper():\n",
        "                    stages[current_stage][\"logic_complexity\"] += 1\n",
        "\n",
        "            # Inputs\n",
        "            input_match = re.search(input_pattern, line)\n",
        "            if input_match:\n",
        "                stages[current_stage][\"inputs\"].append(input_match.group(1))\n",
        "\n",
        "            # Outputs\n",
        "            output_match = re.search(output_pattern, line)\n",
        "            if output_match:\n",
        "                dataset = output_match.group(1)\n",
        "                stages[current_stage][\"outputs\"].append(dataset)\n",
        "                dataset_producers[dataset] = current_stage\n",
        "\n",
        "            # Stage variables\n",
        "            stagevar_match = re.search(stagevar_pattern, line)\n",
        "            if stagevar_match:\n",
        "                stages[current_stage][\"stagevars\"].append(stagevar_match.group(1))\n",
        "\n",
        "            # Constraints\n",
        "            constraint_match = re.search(constraint_pattern, line)\n",
        "            if constraint_match:\n",
        "                stages[current_stage][\"constraints\"].append(constraint_match.group(2))\n",
        "\n",
        "    return stages, dataset_producers\n",
        "\n",
        "\n",
        "# =====================================================\n",
        "# 3Ô∏è‚É£ COMPUTE EXECUTION DEPTH\n",
        "# =====================================================\n",
        "\n",
        "def compute_depths(stages, dataset_producers):\n",
        "\n",
        "    graph = defaultdict(list)\n",
        "    indegree = defaultdict(int)\n",
        "\n",
        "    for stage, info in stages.items():\n",
        "        for dataset in info[\"inputs\"]:\n",
        "            if dataset in dataset_producers:\n",
        "                parent = dataset_producers[dataset]\n",
        "                graph[parent].append(stage)\n",
        "                indegree[stage] += 1\n",
        "\n",
        "    depth = {}\n",
        "    queue = deque()\n",
        "\n",
        "    for stage in stages:\n",
        "        if indegree[stage] == 0:\n",
        "            queue.append(stage)\n",
        "            depth[stage] = 0\n",
        "\n",
        "    while queue:\n",
        "        node = queue.popleft()\n",
        "        for neighbor in graph[node]:\n",
        "            indegree[neighbor] -= 1\n",
        "            if indegree[neighbor] == 0:\n",
        "                depth[neighbor] = depth[node] + 1\n",
        "                queue.append(neighbor)\n",
        "\n",
        "    return depth\n",
        "\n",
        "\n",
        "# =====================================================\n",
        "# 4Ô∏è‚É£ BUILD GRAPHVIZ FROM METADATA\n",
        "# =====================================================\n",
        "\n",
        "def build_graphviz(stages, dataset_producers):\n",
        "\n",
        "    dot = Digraph(\"ETL_Flow\", engine=\"dot\")\n",
        "    dot.attr(rankdir=\"LR\", splines=\"spline\", nodesep=\"0.8\", ranksep=\"1.2\")\n",
        "\n",
        "    dot.attr(\"node\",\n",
        "             shape=\"box\",\n",
        "             style=\"rounded,filled\",\n",
        "             width=\"3\",\n",
        "             height=\"1\")\n",
        "\n",
        "    depths = compute_depths(stages, dataset_producers)\n",
        "\n",
        "    # Detect final targets\n",
        "    outgoing = defaultdict(int)\n",
        "    for stage, info in stages.items():\n",
        "        for dataset in info[\"inputs\"]:\n",
        "            if dataset in dataset_producers:\n",
        "                parent = dataset_producers[dataset]\n",
        "                outgoing[parent] += 1\n",
        "\n",
        "    targets = [s for s in stages if outgoing[s] == 0]\n",
        "\n",
        "    max_depth = max(depths.values())\n",
        "    for t in targets:\n",
        "        if stages[t][\"layer\"] == \"DB\":\n",
        "            depths[t] = max_depth + 1\n",
        "\n",
        "    # Group by depth\n",
        "    levels = defaultdict(list)\n",
        "    for stage, d in depths.items():\n",
        "        levels[d].append(stage)\n",
        "\n",
        "    # Create nodes\n",
        "    for d in sorted(levels):\n",
        "        with dot.subgraph() as s:\n",
        "            s.attr(rank=\"same\")\n",
        "            for stage in levels[d]:\n",
        "                info = stages[stage]\n",
        "\n",
        "                label = f\"{stage}\\n\"\n",
        "                label += f\"Type: {info['type']}\\n\"\n",
        "                label += f\"In: {len(info['inputs'])} | Out: {len(info['outputs'])}\\n\"\n",
        "\n",
        "                if info[\"tables\"]:\n",
        "                    label += f\"Tables: {len(set(info['tables']))}\\n\"\n",
        "\n",
        "                if info[\"joins\"] > 0:\n",
        "                    label += f\"Joins: {info['joins']}\\n\"\n",
        "\n",
        "                if info[\"stagevars\"]:\n",
        "                    label += f\"StageVars: {len(info['stagevars'])}\\n\"\n",
        "\n",
        "                if info[\"constraints\"]:\n",
        "                    label += f\"Constraints: {len(info['constraints'])}\"\n",
        "\n",
        "                color = \"#F4F6F7\"\n",
        "                if info[\"layer\"] == \"DB\":\n",
        "                    color = \"#AED6F1\"\n",
        "                elif info[\"layer\"] == \"TRANSFORM\":\n",
        "                    color = \"#F9E79F\"\n",
        "                elif info[\"layer\"] == \"HASH\":\n",
        "                    color = \"#ABEBC6\"\n",
        "                elif info[\"layer\"] == \"FILE\":\n",
        "                    color = \"#F5B7B1\"\n",
        "\n",
        "                s.node(stage, label, fillcolor=color)\n",
        "\n",
        "    # Add edges\n",
        "    for stage, info in stages.items():\n",
        "        for dataset in info[\"inputs\"]:\n",
        "            if dataset in dataset_producers:\n",
        "                parent = dataset_producers[dataset]\n",
        "\n",
        "                if stages[parent][\"layer\"] == \"HASH\":\n",
        "                    dot.edge(parent, stage,\n",
        "                             constraint=\"false\",\n",
        "                             color=\"gray\")\n",
        "                else:\n",
        "                    dot.edge(parent, stage)\n",
        "\n",
        "    return dot\n",
        "\n",
        "\n",
        "# =====================================================\n",
        "# 5Ô∏è‚É£ RUN + EXPORT\n",
        "# =====================================================\n",
        "\n",
        "with open(\"Samle_Job2 1 2_detailed_pseudocode.txt\", \"r\") as f:\n",
        "    pseudo_text = f.read()\n",
        "\n",
        "stages, producers = parse_pseudocode(pseudo_text)\n",
        "\n",
        "# Export JSON metadata\n",
        "metadata = {\n",
        "    \"stages\": stages,\n",
        "    \"dependencies\": producers\n",
        "}\n",
        "\n",
        "with open(\"etl_metadata.json\", \"w\") as f:\n",
        "    json.dump(metadata, f, indent=4)\n",
        "\n",
        "# Build diagram\n",
        "graph = build_graphviz(stages, producers)\n",
        "graph.render(\"etl_flow_detailed\", format=\"pdf\")\n",
        "\n",
        "print(\"‚úÖ JSON exported as etl_metadata.json\")\n",
        "print(\"‚úÖ Diagram generated as etl_flow_detailed.pdf\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import json\n",
        "from collections import defaultdict\n",
        "\n",
        "\n",
        "# ==========================================================\n",
        "# 1Ô∏è‚É£ CLASSIFY STAGE LAYER\n",
        "# ==========================================================\n",
        "\n",
        "def classify_layer(stage_type):\n",
        "    st = stage_type.upper()\n",
        "\n",
        "    if \"TRANSFORMER\" in st:\n",
        "        return \"Transform\"\n",
        "\n",
        "    if \"HASHED\" in st:\n",
        "        return \"Intermediate\"\n",
        "\n",
        "    if \"SEQ\" in st:\n",
        "        return \"Outputs\"\n",
        "\n",
        "    if \"ORACLE\" in st or \"CUSTOM\" in st:\n",
        "        return \"Source_Target\"\n",
        "\n",
        "    return \"Other\"\n",
        "\n",
        "\n",
        "# ==========================================================\n",
        "# 2Ô∏è‚É£ PARSE PSEUDOCODE\n",
        "# ==========================================================\n",
        "\n",
        "def parse_pseudocode(text):\n",
        "\n",
        "    stages = {}\n",
        "    dataset_producers = {}\n",
        "\n",
        "    stage_pattern = r\"--- \\[(.*?) : (.*?)\\]\"\n",
        "    input_pattern = r\"Input:\\s*‚Üê\\s*(dataset_\\d+)\\s*\\((.*?)\\)\"\n",
        "    output_pattern = r\"Output:\\s*‚Üí\\s*(dataset_\\d+)\\s*\\((.*?)\\)\"\n",
        "    stagevar_pattern = r\"StageVar (.*)\"\n",
        "    constraint_pattern = r\"Constraint \\((.*?)\\): (.*)\"\n",
        "    table_pattern = r\"\\bFROM\\s+([\\w\\.]+)|\\bJOIN\\s+([\\w\\.]+)\"\n",
        "\n",
        "    current_stage = None\n",
        "    collecting_sql = False\n",
        "\n",
        "    for line in text.splitlines():\n",
        "\n",
        "        stage_match = re.search(stage_pattern, line)\n",
        "        if stage_match:\n",
        "            stage_type = stage_match.group(1).strip()\n",
        "            stage_name = stage_match.group(2).strip()\n",
        "\n",
        "            current_stage = stage_name\n",
        "            collecting_sql = False\n",
        "\n",
        "            stages[current_stage] = {\n",
        "                \"type\": stage_type,\n",
        "                \"layer\": classify_layer(stage_type),\n",
        "                \"inputs\": [],\n",
        "                \"outputs\": [],\n",
        "                \"stagevars\": [],\n",
        "                \"constraints\": [],\n",
        "                \"tables\": [],\n",
        "                \"joins\": 0,\n",
        "                \"business_rules\": []\n",
        "            }\n",
        "            continue\n",
        "\n",
        "        if not current_stage:\n",
        "            continue\n",
        "\n",
        "        if \"SQL:\" in line:\n",
        "            collecting_sql = True\n",
        "            continue\n",
        "\n",
        "        if collecting_sql:\n",
        "            tables = re.findall(table_pattern, line, re.IGNORECASE)\n",
        "            for t in tables:\n",
        "                for table in t:\n",
        "                    if table:\n",
        "                        stages[current_stage][\"tables\"].append(table)\n",
        "\n",
        "            if \"JOIN\" in line.upper():\n",
        "                stages[current_stage][\"joins\"] += 1\n",
        "\n",
        "        input_match = re.search(input_pattern, line)\n",
        "        if input_match:\n",
        "            dataset = input_match.group(1)\n",
        "            dataset_name = input_match.group(2)\n",
        "            stages[current_stage][\"inputs\"].append((dataset, dataset_name))\n",
        "\n",
        "        output_match = re.search(output_pattern, line)\n",
        "        if output_match:\n",
        "            dataset = output_match.group(1)\n",
        "            dataset_name = output_match.group(2)\n",
        "            stages[current_stage][\"outputs\"].append((dataset, dataset_name))\n",
        "            dataset_producers[dataset] = current_stage\n",
        "\n",
        "        stagevar_match = re.search(stagevar_pattern, line)\n",
        "        if stagevar_match:\n",
        "            rule = stagevar_match.group(1).strip()\n",
        "            stages[current_stage][\"stagevars\"].append(rule)\n",
        "            stages[current_stage][\"business_rules\"].append(rule)\n",
        "\n",
        "        constraint_match = re.search(constraint_pattern, line)\n",
        "        if constraint_match:\n",
        "            rule = constraint_match.group(2).strip()\n",
        "            stages[current_stage][\"constraints\"].append(rule)\n",
        "            stages[current_stage][\"business_rules\"].append(rule)\n",
        "\n",
        "        if \" IF \" in line.upper():\n",
        "            stages[current_stage][\"business_rules\"].append(line.strip())\n",
        "\n",
        "    return stages, dataset_producers\n",
        "\n",
        "\n",
        "# ==========================================================\n",
        "# 3Ô∏è‚É£ GENERATE MERMAID CODE\n",
        "# ==========================================================\n",
        "\n",
        "def generate_mermaid(stages, producers):\n",
        "\n",
        "    lines = []\n",
        "    lines.append(\"flowchart LR\")\n",
        "\n",
        "    layers = defaultdict(list)\n",
        "    for stage, info in stages.items():\n",
        "        layers[info[\"layer\"]].append(stage)\n",
        "\n",
        "    for layer, stage_list in layers.items():\n",
        "        lines.append(f\"  subgraph {layer}\")\n",
        "        for stage in stage_list:\n",
        "            info = stages[stage]\n",
        "\n",
        "            label = f\"{info['type']}: {stage}\"\n",
        "            if info[\"joins\"] > 0:\n",
        "                label += f\"<br/>Joins: {info['joins']}\"\n",
        "            if info[\"stagevars\"]:\n",
        "                label += f\"<br/>StageVars: {len(info['stagevars'])}\"\n",
        "            if info[\"constraints\"]:\n",
        "                label += f\"<br/>Constraints: {len(info['constraints'])}\"\n",
        "\n",
        "            lines.append(f'    {stage}[\"{label}\"]')\n",
        "        lines.append(\"  end\\n\")\n",
        "\n",
        "    for stage, info in stages.items():\n",
        "        for dataset, dataset_name in info[\"inputs\"]:\n",
        "            parent = producers.get(dataset)\n",
        "            if parent:\n",
        "                lines.append(\n",
        "                    f'  {parent} -- \"{dataset}: {dataset_name}\" --> {stage}'\n",
        "                )\n",
        "\n",
        "    return \"\\n\".join(lines)\n",
        "\n",
        "\n",
        "# ==========================================================\n",
        "# 4Ô∏è‚É£ EXPORT TO PDF-READY HTML\n",
        "# ==========================================================\n",
        "\n",
        "with open(\"Sample_Job1 1 2_detailed_pseudocode.txt\", \"r\") as f:\n",
        "    pseudo_text = f.read()\n",
        "\n",
        "stages, producers = parse_pseudocode(pseudo_text)\n",
        "mermaid_code = generate_mermaid(stages, producers)\n",
        "\n",
        "html_template = f\"\"\"\n",
        "<!DOCTYPE html>\n",
        "<html>\n",
        "<head>\n",
        "  <script src=\"https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js\"></script>\n",
        "  <script>mermaid.initialize({{ startOnLoad: true }});</script>\n",
        "</head>\n",
        "<body>\n",
        "<div class=\"mermaid\">\n",
        "{mermaid_code}\n",
        "</div>\n",
        "</body>\n",
        "</html>\n",
        "\"\"\"\n",
        "\n",
        "with open(\"etl_flow.html\", \"w\") as f:\n",
        "    f.write(html_template)\n",
        "\n",
        "print(\"HTML generated ‚Üí etl_flow.html\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GMZRcFCiyFju",
        "outputId": "21edf3fe-ac62-42e0-a4ae-3e8920df5036"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ HTML generated ‚Üí etl_flow.html\n",
            "üëâ Open in browser ‚Üí Print ‚Üí Save as PDF\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import json\n",
        "import os\n",
        "from collections import defaultdict\n",
        "from dotenv import load_dotenv\n",
        "from openai import OpenAI\n",
        "\n",
        "# ==========================================================\n",
        "# üîê LOAD API KEY\n",
        "# ==========================================================\n",
        "\n",
        "load_dotenv()\n",
        "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
        "\n",
        "# ==========================================================\n",
        "# 1Ô∏è‚É£ CLASSIFY LAYER\n",
        "# ==========================================================\n",
        "\n",
        "def classify_layer(stage_type):\n",
        "    st = stage_type.upper()\n",
        "\n",
        "    if \"TRANSFORMER\" in st:\n",
        "        return \"Transform\"\n",
        "\n",
        "    if \"HASHED\" in st:\n",
        "        return \"Intermediate\"\n",
        "\n",
        "    if \"SEQ\" in st:\n",
        "        return \"Outputs\"\n",
        "\n",
        "    if \"ORACLE\" in st or \"CUSTOM\" in st:\n",
        "        return \"Source_Target\"\n",
        "\n",
        "    return \"Other\"\n",
        "\n",
        "\n",
        "# ==========================================================\n",
        "# 2Ô∏è‚É£ DETERMINISTIC PARSER\n",
        "# ==========================================================\n",
        "\n",
        "def parse_pseudocode(text):\n",
        "\n",
        "    stages = {}\n",
        "    dataset_producers = {}\n",
        "\n",
        "    stage_pattern = r\"--- \\[(.*?) : (.*?)\\]\"\n",
        "    input_pattern = r\"Input:\\s*‚Üê\\s*(dataset_\\d+)\\s*\\((.*?)\\)\"\n",
        "    output_pattern = r\"Output:\\s*‚Üí\\s*(dataset_\\d+)\\s*\\((.*?)\\)\"\n",
        "    stagevar_pattern = r\"StageVar (.*)\"\n",
        "    constraint_pattern = r\"Constraint \\((.*?)\\): (.*)\"\n",
        "    table_pattern = r\"\\bFROM\\s+([\\w\\.]+)|\\bJOIN\\s+([\\w\\.]+)\"\n",
        "\n",
        "    current_stage = None\n",
        "    collecting_sql = False\n",
        "\n",
        "    for line in text.splitlines():\n",
        "\n",
        "        stage_match = re.search(stage_pattern, line)\n",
        "        if stage_match:\n",
        "            stage_type = stage_match.group(1).strip()\n",
        "            stage_name = stage_match.group(2).strip()\n",
        "\n",
        "            current_stage = stage_name\n",
        "            collecting_sql = False\n",
        "\n",
        "            stages[current_stage] = {\n",
        "                \"type\": stage_type,\n",
        "                \"layer\": classify_layer(stage_type),\n",
        "                \"inputs\": [],\n",
        "                \"outputs\": [],\n",
        "                \"stagevars\": [],\n",
        "                \"constraints\": [],\n",
        "                \"tables\": [],\n",
        "                \"joins\": 0\n",
        "            }\n",
        "            continue\n",
        "\n",
        "        if not current_stage:\n",
        "            continue\n",
        "\n",
        "        if \"SQL:\" in line:\n",
        "            collecting_sql = True\n",
        "            continue\n",
        "\n",
        "        if collecting_sql:\n",
        "            tables = re.findall(table_pattern, line, re.IGNORECASE)\n",
        "            for t in tables:\n",
        "                for table in t:\n",
        "                    if table:\n",
        "                        stages[current_stage][\"tables\"].append(table)\n",
        "\n",
        "            if \"JOIN\" in line.upper():\n",
        "                stages[current_stage][\"joins\"] += 1\n",
        "\n",
        "        input_match = re.search(input_pattern, line)\n",
        "        if input_match:\n",
        "            dataset = input_match.group(1)\n",
        "            dataset_name = input_match.group(2)\n",
        "            stages[current_stage][\"inputs\"].append({\n",
        "                \"dataset_id\": dataset,\n",
        "                \"dataset_name\": dataset_name\n",
        "            })\n",
        "\n",
        "        output_match = re.search(output_pattern, line)\n",
        "        if output_match:\n",
        "            dataset = output_match.group(1)\n",
        "            dataset_name = output_match.group(2)\n",
        "            stages[current_stage][\"outputs\"].append({\n",
        "                \"dataset_id\": dataset,\n",
        "                \"dataset_name\": dataset_name\n",
        "            })\n",
        "            dataset_producers[dataset] = current_stage\n",
        "\n",
        "        stagevar_match = re.search(stagevar_pattern, line)\n",
        "        if stagevar_match:\n",
        "            stages[current_stage][\"stagevars\"].append(stagevar_match.group(1).strip())\n",
        "\n",
        "        constraint_match = re.search(constraint_pattern, line)\n",
        "        if constraint_match:\n",
        "            stages[current_stage][\"constraints\"].append(constraint_match.group(2).strip())\n",
        "\n",
        "    return stages, dataset_producers\n",
        "\n",
        "\n",
        "# ==========================================================\n",
        "# 3Ô∏è‚É£ LLM ENRICHMENT\n",
        "# ==========================================================\n",
        "\n",
        "def enrich_stage_with_llm(stage_name, stage_metadata):\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "You are an expert ETL architecture analyst.\n",
        "\n",
        "Analyze the following stage metadata and return JSON only with:\n",
        "\n",
        "- business_summary\n",
        "- stage_role (fact_load, dimension_load, enrichment, staging, file_output, lookup)\n",
        "- exception_logic\n",
        "- lookup_logic\n",
        "- data_enrichment_purpose\n",
        "\n",
        "Metadata:\n",
        "{json.dumps(stage_metadata, indent=2)}\n",
        "\"\"\"\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        temperature=0,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a precise ETL analyzer.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    content = response.choices[0].message.content\n",
        "\n",
        "    try:\n",
        "        return json.loads(content)\n",
        "    except:\n",
        "        return {\"business_summary\": content}\n",
        "\n",
        "\n",
        "def enrich_all_stages(metadata):\n",
        "\n",
        "    for stage, info in metadata[\"stages\"].items():\n",
        "        print(f\"üîç Enriching {stage} ...\")\n",
        "        enriched = enrich_stage_with_llm(stage, info)\n",
        "        metadata[\"stages\"][stage][\"semantic\"] = enriched\n",
        "\n",
        "    return metadata\n",
        "\n",
        "\n",
        "# ==========================================================\n",
        "# 4Ô∏è‚É£ GENERATE MERMAID\n",
        "# ==========================================================\n",
        "\n",
        "def generate_mermaid(metadata):\n",
        "\n",
        "    stages = metadata[\"stages\"]\n",
        "    producers = metadata[\"dependencies\"]\n",
        "\n",
        "    lines = []\n",
        "    lines.append(\"flowchart LR\\n\")\n",
        "\n",
        "    layers = defaultdict(list)\n",
        "    for stage, info in stages.items():\n",
        "        layers[info[\"layer\"]].append(stage)\n",
        "\n",
        "    for layer, stage_list in layers.items():\n",
        "        lines.append(f\"  subgraph {layer}\")\n",
        "        for stage in stage_list:\n",
        "            info = stages[stage]\n",
        "\n",
        "            label = f\"{info['type']}: {stage}\"\n",
        "\n",
        "            if \"semantic\" in info:\n",
        "                summary = info[\"semantic\"].get(\"business_summary\", \"\")\n",
        "                summary = summary.replace('\"', \"'\")\n",
        "                label += f\"<br/><i>{summary}</i>\"\n",
        "\n",
        "            lines.append(f'    {stage}[\"{label}\"]')\n",
        "        lines.append(\"  end\\n\")\n",
        "\n",
        "    for stage, info in stages.items():\n",
        "        for inp in info[\"inputs\"]:\n",
        "            dataset_id = inp[\"dataset_id\"]\n",
        "            dataset_name = inp[\"dataset_name\"]\n",
        "            parent = producers.get(dataset_id)\n",
        "\n",
        "            if parent:\n",
        "                lines.append(\n",
        "                    f'  {parent} -- \"{dataset_id}: {dataset_name}\" --> {stage}'\n",
        "                )\n",
        "\n",
        "    return \"\\n\".join(lines)\n",
        "\n",
        "\n",
        "# ==========================================================\n",
        "# 5Ô∏è‚É£ MAIN EXECUTION\n",
        "# ==========================================================\n",
        "\n",
        "with open(\"Sample_job1 1 2_detailed_pseudocode.txt\", \"r\") as f:\n",
        "    pseudo_text = f.read()\n",
        "\n",
        "stages, producers = parse_pseudocode(pseudo_text)\n",
        "\n",
        "metadata = {\n",
        "    \"stages\": stages,\n",
        "    \"dependencies\": producers\n",
        "}\n",
        "\n",
        "# üî¨ LLM enrichment (research phase)\n",
        "metadata = enrich_all_stages(metadata)\n",
        "\n",
        "# Save enriched JSON\n",
        "with open(\"etl_metadata_enriched.json\", \"w\") as f:\n",
        "    json.dump(metadata, f, indent=4)\n",
        "\n",
        "# Generate Mermaid\n",
        "mermaid_code = generate_mermaid(metadata)\n",
        "\n",
        "# Wrap in HTML for PDF\n",
        "html = f\"\"\"\n",
        "<!DOCTYPE html>\n",
        "<html>\n",
        "<head>\n",
        "<script src=\"https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js\"></script>\n",
        "<script>mermaid.initialize({{startOnLoad:true}});</script>\n",
        "</head>\n",
        "<body>\n",
        "<div class=\"mermaid\">\n",
        "{mermaid_code}\n",
        "</div>\n",
        "</body>\n",
        "</html>\n",
        "\"\"\"\n",
        "\n",
        "with open(\"etl_flow_ai.html\", \"w\") as f:\n",
        "    f.write(html)\n",
        "\n",
        "print(\"‚úÖ Enriched metadata ‚Üí etl_metadata_enriched.json\")\n",
        "print(\"‚úÖ Mermaid HTML ‚Üí etl_flow_ai.html\")\n",
        "print(\"üëâ Open HTML in browser ‚Üí Print ‚Üí Save as PDF\")\n"
      ],
      "metadata": {
        "id": "CGRHXpfiyGF0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#analyze_job.py\n",
        "import os\n",
        "import json\n",
        "import re\n",
        "from dotenv import load_dotenv\n",
        "from openai import AzureOpenAI\n",
        "\n",
        "# Load env\n",
        "load_dotenv()\n",
        "\n",
        "client = AzureOpenAI(\n",
        "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
        "    api_version=os.getenv(\"AZURE_OPENAI_API_VERSION\"),\n",
        "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
        ")\n",
        "\n",
        "DEPLOYMENT = os.getenv(\"AZURE_OPENAI_DEPLOYMENT\")\n",
        "\n",
        "\n",
        "def safe_parse(content):\n",
        "    content = content.strip()\n",
        "\n",
        "    # remove markdown\n",
        "    content = re.sub(r\"```json\", \"\", content)\n",
        "    content = re.sub(r\"```\", \"\", content)\n",
        "\n",
        "    try:\n",
        "        return json.loads(content)\n",
        "    except:\n",
        "        match = re.search(r\"\\{.*\\}\", content, re.DOTALL)\n",
        "        if match:\n",
        "            return json.loads(match.group())\n",
        "        raise Exception(\"JSON Parse Failed\")\n",
        "\n",
        "\n",
        "def analyze_full_job(text):\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "You are an ETL reverse engineering expert.\n",
        "\n",
        "Analyze this entire job definition and extract logical pipeline architecture.\n",
        "\n",
        "Return STRICT JSON:\n",
        "\n",
        "{{\n",
        "  \"job_name\": \"\",\n",
        "  \"pipelines\": [\n",
        "    {{\n",
        "      \"name\": \"\",\n",
        "      \"inputs\": [],\n",
        "      \"processes\": [],\n",
        "      \"joins\": [],\n",
        "      \"analytics\": [],\n",
        "      \"outputs\": []\n",
        "    }}\n",
        "  ]\n",
        "}}\n",
        "\n",
        "Rules:\n",
        "- Group stages logically.\n",
        "- Detect exception branches.\n",
        "- Extract join types and join keys.\n",
        "- Extract derived column logic.\n",
        "- Extract output targets.\n",
        "- Keep it structured and concise.\n",
        "- Do not hallucinate.\n",
        "\"\"\"\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=DEPLOYMENT,\n",
        "        temperature=0,\n",
        "        response_format={\"type\": \"json_object\"},\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"Return JSON only.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt + \"\\n\\nJOB:\\n\" + text}\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    return safe_parse(response.choices[0].message.content)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    with open(\"Sample_job.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "        job_text = f.read()\n",
        "\n",
        "    result = analyze_full_job(job_text)\n",
        "\n",
        "    with open(\"etl_pipeline_model.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(result, f, indent=4)\n",
        "\n",
        "    print(\"‚úÖ Pipeline model saved ‚Üí etl_pipeline_model.json\")\n"
      ],
      "metadata": {
        "id": "bSeroZPfyGI4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#render_diagram\n",
        "\n",
        "import json\n",
        "\n",
        "with open(\"etl_pipeline_model.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    model = json.load(f)\n",
        "\n",
        "\n",
        "def build_mermaid(model):\n",
        "\n",
        "    lines = []\n",
        "    lines.append(\"flowchart LR\\n\")\n",
        "\n",
        "    # Color classes\n",
        "    lines.append(\"classDef input fill:#e6f2ff;\")\n",
        "    lines.append(\"classDef process fill:#e6ffe6;\")\n",
        "    lines.append(\"classDef join fill:#fff2cc;\")\n",
        "    lines.append(\"classDef analytics fill:#f4e6ff;\")\n",
        "    lines.append(\"classDef output fill:#e6ffff;\\n\")\n",
        "\n",
        "    for idx, pipeline in enumerate(model[\"pipelines\"]):\n",
        "\n",
        "        pname = f\"PIPE_{idx}\"\n",
        "        lines.append(f\"subgraph {pname}[{pipeline['name']}]\")\n",
        "\n",
        "        prev_node = None\n",
        "\n",
        "        # INPUT\n",
        "        for i, inp in enumerate(pipeline[\"inputs\"]):\n",
        "            node = f\"{pname}_IN_{i}\"\n",
        "            lines.append(f'{node}[\"INPUT<br/>{inp}\"]:::input')\n",
        "            if prev_node:\n",
        "                lines.append(f\"{prev_node} --> {node}\")\n",
        "            prev_node = node\n",
        "\n",
        "        # PROCESS\n",
        "        for i, proc in enumerate(pipeline[\"processes\"]):\n",
        "            node = f\"{pname}_PROC_{i}\"\n",
        "            lines.append(f'{node}[\"PROCESS<br/>{proc}\"]:::process')\n",
        "            if prev_node:\n",
        "                lines.append(f\"{prev_node} --> {node}\")\n",
        "            prev_node = node\n",
        "\n",
        "        # JOIN\n",
        "        for i, join in enumerate(pipeline[\"joins\"]):\n",
        "            node = f\"{pname}_JOIN_{i}\"\n",
        "            lines.append(f'{node}[\"JOIN<br/>{join}\"]:::join')\n",
        "            if prev_node:\n",
        "                lines.append(f\"{prev_node} --> {node}\")\n",
        "            prev_node = node\n",
        "\n",
        "        # ANALYTICS\n",
        "        for i, ana in enumerate(pipeline[\"analytics\"]):\n",
        "            node = f\"{pname}_ANA_{i}\"\n",
        "            lines.append(f'{node}[\"ANALYTICS<br/>{ana}\"]:::analytics')\n",
        "            if prev_node:\n",
        "                lines.append(f\"{prev_node} --> {node}\")\n",
        "            prev_node = node\n",
        "\n",
        "        # OUTPUT\n",
        "        for i, out in enumerate(pipeline[\"outputs\"]):\n",
        "            node = f\"{pname}_OUT_{i}\"\n",
        "            lines.append(f'{node}[\"OUTPUT<br/>{out}\"]:::output')\n",
        "            if prev_node:\n",
        "                lines.append(f\"{prev_node} --> {node}\")\n",
        "            prev_node = node\n",
        "\n",
        "        lines.append(\"end\\n\")\n",
        "\n",
        "    return \"\\n\".join(lines)\n",
        "\n",
        "\n",
        "mermaid_code = build_mermaid(model)\n",
        "\n",
        "html = f\"\"\"\n",
        "<!DOCTYPE html>\n",
        "<html>\n",
        "<head>\n",
        "<script src=\"https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js\"></script>\n",
        "<script>\n",
        "mermaid.initialize({{ startOnLoad: true, theme: \"default\" }});\n",
        "</script>\n",
        "</head>\n",
        "<body>\n",
        "\n",
        "<h2>ETL Pipeline Architecture</h2>\n",
        "\n",
        "<div class=\"mermaid\">\n",
        "{mermaid_code}\n",
        "</div>\n",
        "\n",
        "</body>\n",
        "</html>\n",
        "\"\"\"\n",
        "\n",
        "with open(\"etl_pipeline_diagram.html\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(html)\n",
        "\n",
        "print(\"‚úÖ Diagram generated ‚Üí etl_pipeline_diagram.html\")\n"
      ],
      "metadata": {
        "id": "dbCusboXyGMW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}