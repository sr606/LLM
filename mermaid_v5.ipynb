{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOlGauiYmb5a5t4JOczsZ68",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sr606/LLM/blob/main/mermaid_v5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WHUnP5EJ-WWI"
      },
      "outputs": [],
      "source": [
        "fastapi\n",
        "uvicorn\n",
        "python-dotenv\n",
        "openai>=1.0.0\n",
        "graphviz\n",
        "\n",
        "\n",
        "AZURE_OPENAI_API_KEY=your_key\n",
        "AZURE_OPENAI_ENDPOINT=https://your-resource-name.openai.azure.com/\n",
        "AZURE_OPENAI_API_VERSION=2024-02-15-preview\n",
        "AZURE_OPENAI_DEPLOYMENT=your-deployment-name\n",
        "\n",
        "\n",
        "#parser\n",
        "def split_into_stages(text: str):\n",
        "    \"\"\"\n",
        "    Splits pseudocode into stage blocks.\n",
        "    Adjust marker if needed.\n",
        "    \"\"\"\n",
        "    blocks = text.split(\"// --- [\")\n",
        "    return [block.strip() for block in blocks if block.strip()]\n",
        "\n",
        "\n",
        "#llm_service\n",
        "import os\n",
        "import json\n",
        "from dotenv import load_dotenv\n",
        "from openai import AzureOpenAI\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "\n",
        "class LLMService:\n",
        "    def __init__(self):\n",
        "\n",
        "        self.client = AzureOpenAI(\n",
        "            api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
        "            api_version=os.getenv(\"AZURE_OPENAI_API_VERSION\"),\n",
        "            azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
        "        )\n",
        "\n",
        "        self.deployment = os.getenv(\"AZURE_OPENAI_DEPLOYMENT\")\n",
        "\n",
        "        self.system_prompt = \"\"\"\n",
        "You are an ETL Stage Analyzer.\n",
        "\n",
        "Extract:\n",
        "- stage_name\n",
        "- stage_type\n",
        "- short transformation summary (3-5 bullet points)\n",
        "\n",
        "Return valid JSON only in this format:\n",
        "\n",
        "{\n",
        "  \"stage_name\": \"...\",\n",
        "  \"stage_type\": \"...\",\n",
        "  \"summary\": [\n",
        "      \"bullet 1\",\n",
        "      \"bullet 2\",\n",
        "      \"bullet 3\"\n",
        "  ]\n",
        "}\n",
        "\n",
        "Do not include explanations.\n",
        "Do not include markdown.\n",
        "\"\"\"\n",
        "\n",
        "    def analyze_stage(self, stage_block: str):\n",
        "\n",
        "        response = self.client.chat.completions.create(\n",
        "            model=self.deployment,\n",
        "            temperature=0.0,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": self.system_prompt},\n",
        "                {\"role\": \"user\", \"content\": stage_block}\n",
        "            ],\n",
        "        )\n",
        "\n",
        "        content = response.choices[0].message.content.strip()\n",
        "\n",
        "        try:\n",
        "            return json.loads(content)\n",
        "        except Exception:\n",
        "            return {\n",
        "                \"error\": \"Invalid JSON from LLM\",\n",
        "                \"raw_response\": content\n",
        "            }\n",
        "\n",
        "\n",
        "\n",
        "#agent\n",
        "import os\n",
        "import json\n",
        "from parser import split_into_stages\n",
        "from llm_service import LLMService\n",
        "\n",
        "INPUT_PATH = \"../data/input/pseudocode.txt\"\n",
        "OUTPUT_PATH = \"../data/output/metadata.json\"\n",
        "\n",
        "\n",
        "def run_agent():\n",
        "\n",
        "    if not os.path.exists(INPUT_PATH):\n",
        "        print(\"Input file not found.\")\n",
        "        return\n",
        "\n",
        "    with open(INPUT_PATH, \"r\", encoding=\"utf-8\") as f:\n",
        "        pseudocode = f.read()\n",
        "\n",
        "    stage_blocks = split_into_stages(pseudocode)\n",
        "\n",
        "    if not stage_blocks:\n",
        "        print(\"No stage blocks detected.\")\n",
        "        return\n",
        "\n",
        "    llm = LLMService()\n",
        "    results = []\n",
        "\n",
        "    for idx, block in enumerate(stage_blocks):\n",
        "        print(f\"Processing stage {idx+1}/{len(stage_blocks)}\")\n",
        "        result = llm.analyze_stage(block)\n",
        "        results.append(result)\n",
        "\n",
        "    os.makedirs(\"../data/output\", exist_ok=True)\n",
        "\n",
        "    with open(OUTPUT_PATH, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(results, f, indent=4)\n",
        "\n",
        "    print(\"Extraction complete.\")\n",
        "    print(f\"Metadata saved at {OUTPUT_PATH}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_agent()\n",
        "\n",
        "\n",
        "\n",
        "#server\n",
        "from fastapi import FastAPI\n",
        "from agent import run_agent\n",
        "\n",
        "app = FastAPI()\n",
        "\n",
        "@app.post(\"/generate-diagram\")\n",
        "def generate():\n",
        "    run_agent()\n",
        "    return {\"status\": \"success\", \"message\": \"Metadata generated\"}\n",
        "\n",
        "\n",
        "#client\n",
        "import requests\n",
        "\n",
        "response = requests.post(\"http://127.0.0.1:8000/generate-diagram\")\n",
        "\n",
        "print(response.json())\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#graph_model\n",
        "\n",
        "class Node:\n",
        "    def __init__(self, name, stage_type, summary):\n",
        "        self.name = name\n",
        "        self.stage_type = stage_type\n",
        "        self.summary = summary\n",
        "\n",
        "\n",
        "class Graph:\n",
        "    def __init__(self):\n",
        "        self.nodes = {}\n",
        "        self.edges = set()   # (source, target)\n",
        "\n",
        "    def add_node(self, node: Node):\n",
        "        self.nodes[node.name] = node\n",
        "\n",
        "    def add_edge(self, source, target):\n",
        "        if source != target:\n",
        "            self.edges.add((source, target))\n",
        "\n",
        "\n",
        "#graph_builder\n",
        "\n",
        "import re\n",
        "from graph_model import Graph, Node\n",
        "\n",
        "\n",
        "def build_graph(pseudocode_text, stage_metadata_list):\n",
        "    \"\"\"\n",
        "    Builds stage-to-stage graph deterministically.\n",
        "    \"\"\"\n",
        "\n",
        "    graph = Graph()\n",
        "\n",
        "    # -----------------------------\n",
        "    # Extract stage blocks again\n",
        "    # -----------------------------\n",
        "    pattern = r\"// --- \\[(.*?)\\] ---([\\s\\S]*?)(?=// --- \\[|$)\"\n",
        "    matches = re.findall(pattern, pseudocode_text)\n",
        "\n",
        "    dataset_producer = {}  # dataset_name -> stage_name\n",
        "    dataset_consumers = {}  # dataset_name -> [stage_names]\n",
        "\n",
        "    for header, body in matches:\n",
        "\n",
        "        parts = header.split(\":\")\n",
        "        if len(parts) < 2:\n",
        "            continue\n",
        "\n",
        "        stage_name = parts[1].strip()\n",
        "\n",
        "        # Extract inputs\n",
        "        inputs = re.findall(r\"Input:\\s*←\\s*dataset_\\d+\\s*\\((.*?)\\)\", body)\n",
        "\n",
        "        for dataset in inputs:\n",
        "            dataset_consumers.setdefault(dataset, []).append(stage_name)\n",
        "\n",
        "        # Extract outputs\n",
        "        outputs = re.findall(r\"Output:\\s*→\\s*dataset_\\d+\\s*\\((.*?)\\)\", body)\n",
        "\n",
        "        for dataset in outputs:\n",
        "            dataset_producer[dataset] = stage_name\n",
        "\n",
        "    # -----------------------------\n",
        "    # Add Nodes from metadata\n",
        "    # -----------------------------\n",
        "    for stage_data in stage_metadata_list:\n",
        "        if \"error\" in stage_data:\n",
        "            continue\n",
        "\n",
        "        node = Node(\n",
        "            name=stage_data[\"stage_name\"],\n",
        "            stage_type=stage_data[\"stage_type\"],\n",
        "            summary=stage_data[\"summary\"]\n",
        "        )\n",
        "        graph.add_node(node)\n",
        "\n",
        "    # -----------------------------\n",
        "    # Create stage-to-stage edges\n",
        "    # -----------------------------\n",
        "    for dataset, producer in dataset_producer.items():\n",
        "        consumers = dataset_consumers.get(dataset, [])\n",
        "\n",
        "        for consumer in consumers:\n",
        "            graph.add_edge(producer, consumer)\n",
        "\n",
        "    return graph\n",
        "\n",
        "\n",
        "\n",
        "#run_agent()\n",
        "\n",
        "from graph_builder import build_graph\n",
        "\n",
        "\n",
        "def run_agent():\n",
        "\n",
        "    with open(INPUT_PATH, \"r\", encoding=\"utf-8\") as f:\n",
        "        pseudocode = f.read()\n",
        "\n",
        "    stage_blocks = split_into_stages(pseudocode)\n",
        "\n",
        "    llm = LLMService()\n",
        "    metadata_results = []\n",
        "\n",
        "    for block in stage_blocks:\n",
        "        result = llm.analyze_stage(block)\n",
        "        metadata_results.append(result)\n",
        "\n",
        "    # Save metadata\n",
        "    os.makedirs(\"../data/output\", exist_ok=True)\n",
        "\n",
        "    with open(\"../data/output/metadata.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(metadata_results, f, indent=4)\n",
        "\n",
        "    # Build graph\n",
        "    graph = build_graph(pseudocode, metadata_results)\n",
        "\n",
        "    # Print graph info\n",
        "    print(\"Nodes:\")\n",
        "    for node in graph.nodes.values():\n",
        "        print(\"-\", node.name)\n",
        "\n",
        "    print(\"\\nEdges:\")\n",
        "    for edge in graph.edges:\n",
        "        print(\"-\", edge)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Extract link-file bindings\n",
        "\n",
        "link_file_matches = re.findall(r\"Link File \\((.*?)\\):\\s*(.*)\",body)\n",
        "\n",
        "stage_link_files = {}\n",
        "for link_name, file_name in link_file_matches:\n",
        "  stage_link_files[link_name.strip()] = file_name.strip()\n",
        "\n",
        "\n",
        "\n",
        "#pipeline_detector\n",
        "\n",
        "from collections import defaultdict, deque\n",
        "\n",
        "\n",
        "def detect_pipelines(graph):\n",
        "    \"\"\"\n",
        "    Returns list of pipelines.\n",
        "    Each pipeline is a set of stage names.\n",
        "    \"\"\"\n",
        "\n",
        "    # Build adjacency list\n",
        "    adjacency = defaultdict(set)\n",
        "\n",
        "    for source, target in graph.edges:\n",
        "        adjacency[source].add(target)\n",
        "        adjacency[target].add(source)  # Undirected for connectivity\n",
        "\n",
        "    visited = set()\n",
        "    pipelines = []\n",
        "\n",
        "    for node_name in graph.nodes.keys():\n",
        "\n",
        "        if node_name in visited:\n",
        "            continue\n",
        "\n",
        "        queue = deque([node_name])\n",
        "        component = set()\n",
        "\n",
        "        while queue:\n",
        "            current = queue.popleft()\n",
        "\n",
        "            if current in visited:\n",
        "                continue\n",
        "\n",
        "            visited.add(current)\n",
        "            component.add(current)\n",
        "\n",
        "            for neighbor in adjacency[current]:\n",
        "                if neighbor not in visited:\n",
        "                    queue.append(neighbor)\n",
        "\n",
        "        pipelines.append(component)\n",
        "\n",
        "    return pipelines\n",
        "\n",
        "\n",
        "\n",
        "from pipeline_detector import detect_pipelines\n",
        "\n",
        "pipelines = detect_pipelines(graph)\n",
        "\n",
        "print(\"\\nDetected Pipelines:\")\n",
        "for idx, pipeline in enumerate(pipelines, start=1):\n",
        "    print(f\"\\nPipeline {idx}:\")\n",
        "    for stage in pipeline:\n",
        "        print(\"  -\", stage)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#pipeline_name\n",
        "from llm_service import LLMService\n",
        "\n",
        "\n",
        "def generate_pipeline_name(graph, pipeline_stages):\n",
        "    \"\"\"\n",
        "    Hybrid pipeline naming.\n",
        "    pipeline_stages: set of stage names\n",
        "    \"\"\"\n",
        "\n",
        "    # ---------------------------\n",
        "    # Step 1: Deterministic Fallback\n",
        "    # ---------------------------\n",
        "\n",
        "    # Find source stages (no incoming edges)\n",
        "    incoming = {stage: 0 for stage in pipeline_stages}\n",
        "\n",
        "    for source, target in graph.edges:\n",
        "        if target in incoming:\n",
        "            incoming[target] += 1\n",
        "\n",
        "    source_candidates = [stage for stage, count in incoming.items() if count == 0]\n",
        "\n",
        "    if source_candidates:\n",
        "        fallback_name = source_candidates[0]\n",
        "    else:\n",
        "        fallback_name = list(pipeline_stages)[0]\n",
        "\n",
        "    # ---------------------------\n",
        "    # Step 2: Build Structured Summary\n",
        "    # ---------------------------\n",
        "\n",
        "    pipeline_summary = []\n",
        "\n",
        "    for stage in pipeline_stages:\n",
        "        node = graph.nodes.get(stage)\n",
        "        if not node:\n",
        "            continue\n",
        "\n",
        "        pipeline_summary.append({\n",
        "            \"stage_name\": node.name,\n",
        "            \"stage_type\": node.stage_type,\n",
        "            \"summary\": node.summary\n",
        "        })\n",
        "\n",
        "    # ---------------------------\n",
        "    # Step 3: Ask LLM For Better Name\n",
        "    # ---------------------------\n",
        "\n",
        "    llm = LLMService()\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "You are generating a professional ETL pipeline name.\n",
        "\n",
        "Based on the following structured pipeline stages,\n",
        "generate a short professional name (max 6 words).\n",
        "\n",
        "Return ONLY the name as plain text.\n",
        "No explanation.\n",
        "\n",
        "Pipeline Data:\n",
        "{pipeline_summary}\n",
        "\"\"\"\n",
        "\n",
        "    try:\n",
        "        response = llm.client.chat.completions.create(\n",
        "            model=llm.deployment,\n",
        "            temperature=0.0,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You generate concise pipeline names.\"},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ],\n",
        "        )\n",
        "\n",
        "        name = response.choices[0].message.content.strip()\n",
        "\n",
        "        # Basic cleanup\n",
        "        name = name.replace(\"\\n\", \"\").strip()\n",
        "\n",
        "        if len(name) > 2:\n",
        "            return name\n",
        "\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    # ---------------------------\n",
        "    # Step 4: Fallback\n",
        "    # ---------------------------\n",
        "\n",
        "    return fallback_name\n",
        "\n",
        "\n",
        "\n",
        "from pipeline_namer import generate_pipeline_name\n",
        "\n",
        "\n",
        "print(\"\\nDetected Pipelines:\")\n",
        "\n",
        "for idx, pipeline in enumerate(pipelines, start=1):\n",
        "\n",
        "    pipeline_name = generate_pipeline_name(graph, pipeline)\n",
        "\n",
        "    print(f\"\\nPipeline {idx}: {pipeline_name}\")\n",
        "\n",
        "    for stage in pipeline:\n",
        "        print(\"  -\", stage)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#renderer\n",
        "\n",
        "from graphviz import Digraph\n",
        "\n",
        "\n",
        "def render_pipeline_pdf(graph, pipelines, pipeline_names, output_path):\n",
        "    \"\"\"\n",
        "    Renders clustered pipeline diagram into PDF.\n",
        "    \"\"\"\n",
        "\n",
        "    dot = Digraph(\"ETL_Pipelines\", format=\"pdf\")\n",
        "    dot.attr(rankdir=\"LR\", size=\"8,5\")\n",
        "\n",
        "    # -----------------------------------\n",
        "    # Create clusters per pipeline\n",
        "    # -----------------------------------\n",
        "    for idx, pipeline in enumerate(pipelines):\n",
        "\n",
        "        cluster_name = f\"cluster_{idx}\"\n",
        "\n",
        "        with dot.subgraph(name=cluster_name) as sub:\n",
        "\n",
        "            sub.attr(label=pipeline_names[idx], style=\"rounded\")\n",
        "\n",
        "            for stage_name in pipeline:\n",
        "                node = graph.nodes.get(stage_name)\n",
        "\n",
        "                if not node:\n",
        "                    continue\n",
        "\n",
        "                # Build node label\n",
        "                label = f\"{node.name}\\n({node.stage_type})\\n\"\n",
        "\n",
        "                for bullet in node.summary[:4]:\n",
        "                    label += f\"• {bullet}\\n\"\n",
        "\n",
        "                # Optional: include link file bindings\n",
        "                if node.link_files:\n",
        "                    label += \"\\nLink Files:\\n\"\n",
        "                    for link, file in node.link_files.items():\n",
        "                        label += f\"{link} → {file}\\n\"\n",
        "\n",
        "                sub.node(\n",
        "                    node.name,\n",
        "                    label=label,\n",
        "                    shape=\"box\"\n",
        "                )\n",
        "\n",
        "    # -----------------------------------\n",
        "    # Add edges globally\n",
        "    # -----------------------------------\n",
        "    for source, target in graph.edges:\n",
        "        dot.edge(source, target)\n",
        "\n",
        "    # -----------------------------------\n",
        "    # Render file\n",
        "    # -----------------------------------\n",
        "    dot.render(output_path, cleanup=True)\n",
        "\n",
        "\n",
        "from renderer import render_pipeline_pdf\n",
        "\n",
        "pipeline_names = []\n",
        "\n",
        "for pipeline in pipelines:\n",
        "    name = generate_pipeline_name(graph, pipeline)\n",
        "    pipeline_names.append(name)\n",
        "\n",
        "\n",
        "output_file = \"../data/output/etl_pipeline_diagram\"\n",
        "\n",
        "render_pipeline_pdf(\n",
        "    graph,\n",
        "    pipelines,\n",
        "    pipeline_names,\n",
        "    output_file\n",
        ")\n",
        "\n",
        "print(\"\\nPDF Generated at:\")\n",
        "print(\"../data/output/etl_pipeline_diagram.pdf\")\n"
      ],
      "metadata": {
        "id": "PjtG95TfpfcF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dlMDmmd78veb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}